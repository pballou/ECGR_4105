{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPatrick Ballou\\nID: 801130521\\nECGR 4105\\nHomework 6\\nProblem 2\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Patrick Ballou\n",
    "ID: 801130521\n",
    "ECGR 4105\n",
    "Homework 6\n",
    "Problem 2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = 'C:/Users/patri/Documents/School_Fall_2022/ECGR_4105/Homework/homework_6'\n",
    "#normalized\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro T2000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "    loss_train = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "      imgs, labels = imgs.to(device=device), labels.to(device=device)\n",
    "      outputs = model(imgs)\n",
    "      loss = loss_fn(outputs, labels)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss_train += loss.item()\n",
    "    print(\"{} Epoch {}, Training loss {}\".format(datetime.datetime.now(), epoch,loss_train / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_r = .008\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-10 22:08:06.680163 Epoch 1, Training loss 2.1979545046911215\n",
      "2022-12-10 22:08:20.999803 Epoch 2, Training loss 1.9901536742744543\n",
      "2022-12-10 22:08:35.046300 Epoch 3, Training loss 1.8898709930117479\n",
      "2022-12-10 22:08:49.951607 Epoch 4, Training loss 1.801148413392284\n",
      "2022-12-10 22:09:04.162427 Epoch 5, Training loss 1.7086237025687763\n",
      "2022-12-10 22:09:17.786948 Epoch 6, Training loss 1.6298809902137503\n",
      "2022-12-10 22:09:32.236321 Epoch 7, Training loss 1.5677374232455592\n",
      "2022-12-10 22:09:47.156629 Epoch 8, Training loss 1.5179161183974321\n",
      "2022-12-10 22:10:01.271216 Epoch 9, Training loss 1.4756473177839118\n",
      "2022-12-10 22:10:15.797970 Epoch 10, Training loss 1.4370024292670247\n",
      "2022-12-10 22:10:30.462755 Epoch 11, Training loss 1.4003242754265475\n",
      "2022-12-10 22:10:44.658084 Epoch 12, Training loss 1.3669757870457055\n",
      "2022-12-10 22:10:58.969155 Epoch 13, Training loss 1.3368382127693548\n",
      "2022-12-10 22:11:12.816422 Epoch 14, Training loss 1.3108555907788484\n",
      "2022-12-10 22:11:26.728299 Epoch 15, Training loss 1.2857873427593494\n",
      "2022-12-10 22:11:40.789264 Epoch 16, Training loss 1.2639208562538753\n",
      "2022-12-10 22:11:54.106507 Epoch 17, Training loss 1.2448323544333963\n",
      "2022-12-10 22:12:08.256023 Epoch 18, Training loss 1.2258145717708655\n",
      "2022-12-10 22:12:21.504006 Epoch 19, Training loss 1.2086561531057138\n",
      "2022-12-10 22:12:36.324056 Epoch 20, Training loss 1.1964264477000517\n",
      "2022-12-10 22:12:50.786919 Epoch 21, Training loss 1.1810239847663724\n",
      "2022-12-10 22:13:05.798902 Epoch 22, Training loss 1.1685948632562253\n",
      "2022-12-10 22:13:20.619998 Epoch 23, Training loss 1.1573904109427997\n",
      "2022-12-10 22:13:34.882837 Epoch 24, Training loss 1.1459447756752639\n",
      "2022-12-10 22:13:49.430474 Epoch 25, Training loss 1.136744181365918\n",
      "2022-12-10 22:14:03.339374 Epoch 26, Training loss 1.125370956747733\n",
      "2022-12-10 22:14:18.337045 Epoch 27, Training loss 1.1184027234611609\n",
      "2022-12-10 22:14:34.001664 Epoch 28, Training loss 1.1079243503873\n",
      "2022-12-10 22:14:48.126281 Epoch 29, Training loss 1.0995346524221512\n",
      "2022-12-10 22:15:02.856268 Epoch 30, Training loss 1.0906937459240789\n",
      "2022-12-10 22:15:16.276806 Epoch 31, Training loss 1.0833347823918629\n",
      "2022-12-10 22:15:30.758787 Epoch 32, Training loss 1.076526078879071\n",
      "2022-12-10 22:15:49.413934 Epoch 33, Training loss 1.0678015171414446\n",
      "2022-12-10 22:16:05.438520 Epoch 34, Training loss 1.0623739585852074\n",
      "2022-12-10 22:16:23.380188 Epoch 35, Training loss 1.0543872253669193\n",
      "2022-12-10 22:16:36.947118 Epoch 36, Training loss 1.0468636147506403\n",
      "2022-12-10 22:16:51.554948 Epoch 37, Training loss 1.0412665834207364\n",
      "2022-12-10 22:17:04.699326 Epoch 38, Training loss 1.0344944218235552\n",
      "2022-12-10 22:17:19.292014 Epoch 39, Training loss 1.0281944476125184\n",
      "2022-12-10 22:17:41.196621 Epoch 40, Training loss 1.021532031581225\n",
      "2022-12-10 22:17:59.928442 Epoch 41, Training loss 1.0146988281203657\n",
      "2022-12-10 22:18:19.328619 Epoch 42, Training loss 1.008915440048403\n",
      "2022-12-10 22:18:34.200005 Epoch 43, Training loss 1.0042430189869287\n",
      "2022-12-10 22:18:47.708019 Epoch 44, Training loss 0.997811471714693\n",
      "2022-12-10 22:19:01.872980 Epoch 45, Training loss 0.9926243692712711\n",
      "2022-12-10 22:19:14.884974 Epoch 46, Training loss 0.9875839790114966\n",
      "2022-12-10 22:19:28.334993 Epoch 47, Training loss 0.9817948367284692\n",
      "2022-12-10 22:19:43.082071 Epoch 48, Training loss 0.9742717409072934\n",
      "2022-12-10 22:19:59.244216 Epoch 49, Training loss 0.9732209011112027\n",
      "2022-12-10 22:20:19.203158 Epoch 50, Training loss 0.9678986718892442\n",
      "2022-12-10 22:20:31.707270 Epoch 51, Training loss 0.9631868460599113\n",
      "2022-12-10 22:20:44.040776 Epoch 52, Training loss 0.9577572833546593\n",
      "2022-12-10 22:21:03.425553 Epoch 53, Training loss 0.9527728127701508\n",
      "2022-12-10 22:21:16.473547 Epoch 54, Training loss 0.949890485809892\n",
      "2022-12-10 22:21:30.119424 Epoch 55, Training loss 0.9438154971812998\n",
      "2022-12-10 22:21:44.281147 Epoch 56, Training loss 0.9425320382923117\n",
      "2022-12-10 22:21:59.203019 Epoch 57, Training loss 0.9363111132551032\n",
      "2022-12-10 22:22:14.436055 Epoch 58, Training loss 0.9339874049891597\n",
      "2022-12-10 22:22:29.362607 Epoch 59, Training loss 0.9280201696678806\n",
      "2022-12-10 22:22:45.186126 Epoch 60, Training loss 0.9246275603313885\n",
      "2022-12-10 22:22:59.917384 Epoch 61, Training loss 0.9212499852375606\n",
      "2022-12-10 22:23:14.618871 Epoch 62, Training loss 0.9171980117897853\n",
      "2022-12-10 22:23:29.516628 Epoch 63, Training loss 0.9142303870766973\n",
      "2022-12-10 22:23:47.612880 Epoch 64, Training loss 0.9114385114606384\n",
      "2022-12-10 22:24:01.910895 Epoch 65, Training loss 0.9050253667794835\n",
      "2022-12-10 22:24:23.525769 Epoch 66, Training loss 0.901967328985024\n",
      "2022-12-10 22:24:37.936462 Epoch 67, Training loss 0.8994562313379839\n",
      "2022-12-10 22:24:54.549726 Epoch 68, Training loss 0.8959833206727986\n",
      "2022-12-10 22:25:08.129653 Epoch 69, Training loss 0.893583239954146\n",
      "2022-12-10 22:25:21.869233 Epoch 70, Training loss 0.8891387086390229\n",
      "2022-12-10 22:25:35.634832 Epoch 71, Training loss 0.8873978979752192\n",
      "2022-12-10 22:25:53.285491 Epoch 72, Training loss 0.8838397064782164\n",
      "2022-12-10 22:26:06.430543 Epoch 73, Training loss 0.8824207095233986\n",
      "2022-12-10 22:26:22.390293 Epoch 74, Training loss 0.8797528326054058\n",
      "2022-12-10 22:26:36.899620 Epoch 75, Training loss 0.8760131102083893\n",
      "2022-12-10 22:26:53.308536 Epoch 76, Training loss 0.8735984310774547\n",
      "2022-12-10 22:27:08.822245 Epoch 77, Training loss 0.869749308394654\n",
      "2022-12-10 22:27:22.133376 Epoch 78, Training loss 0.8677272726507748\n",
      "2022-12-10 22:27:40.096459 Epoch 79, Training loss 0.8657730691268316\n",
      "2022-12-10 22:27:54.394721 Epoch 80, Training loss 0.8619152383731149\n",
      "2022-12-10 22:28:09.383697 Epoch 81, Training loss 0.8607328241438512\n",
      "2022-12-10 22:28:24.707349 Epoch 82, Training loss 0.8590361451553872\n",
      "2022-12-10 22:28:39.396760 Epoch 83, Training loss 0.854388249347277\n",
      "2022-12-10 22:28:54.527288 Epoch 84, Training loss 0.8511555557665618\n",
      "2022-12-10 22:29:10.595684 Epoch 85, Training loss 0.8501836770330854\n",
      "2022-12-10 22:29:25.212444 Epoch 86, Training loss 0.8472317961780617\n",
      "2022-12-10 22:29:40.056428 Epoch 87, Training loss 0.8463938699658874\n",
      "2022-12-10 22:29:54.720255 Epoch 88, Training loss 0.8420345487497042\n",
      "2022-12-10 22:30:09.454851 Epoch 89, Training loss 0.8415241104257686\n",
      "2022-12-10 22:30:26.086993 Epoch 90, Training loss 0.8403242334075596\n",
      "2022-12-10 22:30:40.874373 Epoch 91, Training loss 0.8371660529500078\n",
      "2022-12-10 22:30:55.641348 Epoch 92, Training loss 0.8347357808781402\n",
      "2022-12-10 22:31:10.377281 Epoch 93, Training loss 0.8332093419016474\n",
      "2022-12-10 22:31:25.024537 Epoch 94, Training loss 0.8309918532286154\n",
      "2022-12-10 22:31:39.496345 Epoch 95, Training loss 0.8285522157578822\n",
      "2022-12-10 22:31:54.327512 Epoch 96, Training loss 0.8271408925580856\n",
      "2022-12-10 22:32:09.017278 Epoch 97, Training loss 0.8255310285731655\n",
      "2022-12-10 22:32:23.666420 Epoch 98, Training loss 0.8221613174814093\n",
      "2022-12-10 22:32:39.416599 Epoch 99, Training loss 0.8212612367347073\n",
      "2022-12-10 22:32:53.696772 Epoch 100, Training loss 0.8183801672647676\n",
      "2022-12-10 22:33:14.028492 Epoch 101, Training loss 0.8173258507343204\n",
      "2022-12-10 22:33:28.851243 Epoch 102, Training loss 0.8170511751528591\n",
      "2022-12-10 22:33:44.228186 Epoch 103, Training loss 0.8137237784807639\n",
      "2022-12-10 22:33:58.952091 Epoch 104, Training loss 0.81224502306765\n",
      "2022-12-10 22:34:13.814647 Epoch 105, Training loss 0.8099683611594197\n",
      "2022-12-10 22:34:29.059173 Epoch 106, Training loss 0.8078798418459685\n",
      "2022-12-10 22:34:43.855005 Epoch 107, Training loss 0.8067192836185856\n",
      "2022-12-10 22:34:57.968485 Epoch 108, Training loss 0.8035710870152544\n",
      "2022-12-10 22:35:12.770074 Epoch 109, Training loss 0.8046963457256326\n",
      "2022-12-10 22:35:27.491580 Epoch 110, Training loss 0.8031961649580075\n",
      "2022-12-10 22:35:42.189005 Epoch 111, Training loss 0.8007764387923433\n",
      "2022-12-10 22:35:56.383048 Epoch 112, Training loss 0.7978661128931948\n",
      "2022-12-10 22:36:10.646493 Epoch 113, Training loss 0.7980888943233149\n",
      "2022-12-10 22:36:25.302837 Epoch 114, Training loss 0.7960916678314014\n",
      "2022-12-10 22:36:39.964201 Epoch 115, Training loss 0.7939233313436094\n",
      "2022-12-10 22:36:54.908853 Epoch 116, Training loss 0.7906477989443123\n",
      "2022-12-10 22:37:09.330581 Epoch 117, Training loss 0.7913113449845472\n",
      "2022-12-10 22:37:23.601257 Epoch 118, Training loss 0.7889630503361792\n",
      "2022-12-10 22:37:38.227730 Epoch 119, Training loss 0.7871620391335938\n",
      "2022-12-10 22:37:52.775927 Epoch 120, Training loss 0.7866725513087515\n",
      "2022-12-10 22:38:07.446027 Epoch 121, Training loss 0.783958613567645\n",
      "2022-12-10 22:38:22.202964 Epoch 122, Training loss 0.781983291249141\n",
      "2022-12-10 22:38:36.792374 Epoch 123, Training loss 0.7833200368429999\n",
      "2022-12-10 22:38:52.898071 Epoch 124, Training loss 0.7798354557103209\n",
      "2022-12-10 22:39:07.552414 Epoch 125, Training loss 0.7777958702858147\n",
      "2022-12-10 22:39:21.769445 Epoch 126, Training loss 0.7777593410990732\n",
      "2022-12-10 22:39:36.447713 Epoch 127, Training loss 0.7754552838442575\n",
      "2022-12-10 22:39:51.252065 Epoch 128, Training loss 0.77553775334907\n",
      "2022-12-10 22:40:06.124413 Epoch 129, Training loss 0.7743174020591599\n",
      "2022-12-10 22:40:20.958528 Epoch 130, Training loss 0.7733361591463503\n",
      "2022-12-10 22:40:36.509950 Epoch 131, Training loss 0.7705572526473219\n",
      "2022-12-10 22:40:50.950472 Epoch 132, Training loss 0.7681149075098355\n",
      "2022-12-10 22:41:05.758301 Epoch 133, Training loss 0.7680025268393709\n",
      "2022-12-10 22:41:20.183344 Epoch 134, Training loss 0.7656788152197133\n",
      "2022-12-10 22:41:34.961360 Epoch 135, Training loss 0.7651608522285891\n",
      "2022-12-10 22:41:49.781566 Epoch 136, Training loss 0.7631373781987163\n",
      "2022-12-10 22:42:04.570921 Epoch 137, Training loss 0.7630967686853141\n",
      "2022-12-10 22:42:19.372732 Epoch 138, Training loss 0.7621128665821632\n",
      "2022-12-10 22:42:34.395225 Epoch 139, Training loss 0.761648403409192\n",
      "2022-12-10 22:42:48.683042 Epoch 140, Training loss 0.7592635121186981\n",
      "2022-12-10 22:43:03.529341 Epoch 141, Training loss 0.7585305066974571\n",
      "2022-12-10 22:43:18.413701 Epoch 142, Training loss 0.7575526891462029\n",
      "2022-12-10 22:43:33.028737 Epoch 143, Training loss 0.7551844475214439\n",
      "2022-12-10 22:43:47.957518 Epoch 144, Training loss 0.7535218671154793\n",
      "2022-12-10 22:44:02.796376 Epoch 145, Training loss 0.7512097086595453\n",
      "2022-12-10 22:44:17.643459 Epoch 146, Training loss 0.7533654139169952\n",
      "2022-12-10 22:44:32.924497 Epoch 147, Training loss 0.7515834368708189\n",
      "2022-12-10 22:44:48.729419 Epoch 148, Training loss 0.7500932866045277\n",
      "2022-12-10 22:45:03.034532 Epoch 149, Training loss 0.7477372151506526\n",
      "2022-12-10 22:45:17.986323 Epoch 150, Training loss 0.7467956146620729\n",
      "2022-12-10 22:45:32.787626 Epoch 151, Training loss 0.7460789610357845\n",
      "2022-12-10 22:45:47.665636 Epoch 152, Training loss 0.7451560876863387\n",
      "2022-12-10 22:46:03.699354 Epoch 153, Training loss 0.743772634917208\n",
      "2022-12-10 22:46:18.058184 Epoch 154, Training loss 0.7428355259663614\n",
      "2022-12-10 22:46:32.549442 Epoch 155, Training loss 0.7418475475762506\n",
      "2022-12-10 22:46:46.867591 Epoch 156, Training loss 0.7410871953610569\n",
      "2022-12-10 22:47:01.238979 Epoch 157, Training loss 0.7399053609432162\n",
      "2022-12-10 22:47:15.763578 Epoch 158, Training loss 0.7397405730793848\n",
      "2022-12-10 22:47:30.585082 Epoch 159, Training loss 0.7371274336524631\n",
      "2022-12-10 22:47:44.896857 Epoch 160, Training loss 0.7365744237399772\n",
      "2022-12-10 22:47:59.040838 Epoch 161, Training loss 0.7354683972075772\n",
      "2022-12-10 22:48:14.681737 Epoch 162, Training loss 0.7332872788009741\n",
      "2022-12-10 22:48:29.033240 Epoch 163, Training loss 0.7322635092698705\n",
      "2022-12-10 22:48:44.656685 Epoch 164, Training loss 0.7328461699778467\n",
      "2022-12-10 22:48:59.412685 Epoch 165, Training loss 0.731083347974226\n",
      "2022-12-10 22:49:14.400917 Epoch 166, Training loss 0.7290349061531789\n",
      "2022-12-10 22:49:28.589494 Epoch 167, Training loss 0.7291350201572604\n",
      "2022-12-10 22:49:43.988289 Epoch 168, Training loss 0.7285049870191023\n",
      "2022-12-10 22:49:58.706647 Epoch 169, Training loss 0.7266134990145788\n",
      "2022-12-10 22:50:14.163907 Epoch 170, Training loss 0.7264001356518787\n",
      "2022-12-10 22:50:32.122319 Epoch 171, Training loss 0.7258329165866003\n",
      "2022-12-10 22:50:46.979863 Epoch 172, Training loss 0.7249765178888959\n",
      "2022-12-10 22:51:01.614503 Epoch 173, Training loss 0.7220467717751212\n",
      "2022-12-10 22:51:16.249574 Epoch 174, Training loss 0.7228345202515497\n",
      "2022-12-10 22:51:30.845285 Epoch 175, Training loss 0.7215506510661386\n",
      "2022-12-10 22:51:45.500315 Epoch 176, Training loss 0.7187476963795665\n",
      "2022-12-10 22:51:57.414867 Epoch 177, Training loss 0.7187671974644332\n",
      "2022-12-10 22:52:11.698212 Epoch 178, Training loss 0.7165605661356845\n",
      "2022-12-10 22:52:25.897668 Epoch 179, Training loss 0.7172389717967919\n",
      "2022-12-10 22:52:40.372721 Epoch 180, Training loss 0.7148659020433645\n",
      "2022-12-10 22:52:54.513600 Epoch 181, Training loss 0.7147455934978202\n",
      "2022-12-10 22:53:08.072206 Epoch 182, Training loss 0.7140742255293805\n",
      "2022-12-10 22:53:22.168427 Epoch 183, Training loss 0.7130409482190067\n",
      "2022-12-10 22:53:35.636217 Epoch 184, Training loss 0.7117919440159712\n",
      "2022-12-10 22:53:49.157915 Epoch 185, Training loss 0.7116221034008524\n",
      "2022-12-10 22:54:03.093955 Epoch 186, Training loss 0.70905413537684\n",
      "2022-12-10 22:54:18.026474 Epoch 187, Training loss 0.707862034478151\n",
      "2022-12-10 22:54:31.516229 Epoch 188, Training loss 0.7076914501007255\n",
      "2022-12-10 22:54:44.474354 Epoch 189, Training loss 0.7057182843727834\n",
      "2022-12-10 22:54:58.226447 Epoch 190, Training loss 0.7070089585488409\n",
      "2022-12-10 22:55:10.636535 Epoch 191, Training loss 0.7053657355515853\n",
      "2022-12-10 22:55:23.469095 Epoch 192, Training loss 0.7033409669880977\n",
      "2022-12-10 22:55:37.734690 Epoch 193, Training loss 0.7033262212410607\n",
      "2022-12-10 22:55:52.342261 Epoch 194, Training loss 0.703243255081689\n",
      "2022-12-10 22:56:07.064076 Epoch 195, Training loss 0.7017450934785712\n",
      "2022-12-10 22:56:21.721005 Epoch 196, Training loss 0.7004739230551074\n",
      "2022-12-10 22:56:34.690320 Epoch 197, Training loss 0.6993022136523596\n",
      "2022-12-10 22:56:49.009740 Epoch 198, Training loss 0.6992099891843089\n",
      "2022-12-10 22:57:02.974987 Epoch 199, Training loss 0.6987519695630768\n",
      "2022-12-10 22:57:16.736514 Epoch 200, Training loss 0.6977403518336508\n",
      "2022-12-10 22:57:30.620580 Epoch 201, Training loss 0.6964365162355516\n",
      "2022-12-10 22:57:44.544555 Epoch 202, Training loss 0.6944650464960377\n",
      "2022-12-10 22:57:58.779104 Epoch 203, Training loss 0.6953029749186143\n",
      "2022-12-10 22:58:12.306321 Epoch 204, Training loss 0.6940501781223375\n",
      "2022-12-10 22:58:26.113137 Epoch 205, Training loss 0.6926899609510856\n",
      "2022-12-10 22:58:39.950519 Epoch 206, Training loss 0.691700072544615\n",
      "2022-12-10 22:58:54.339623 Epoch 207, Training loss 0.691831220217678\n",
      "2022-12-10 22:59:08.132211 Epoch 208, Training loss 0.6911317015547886\n",
      "2022-12-10 22:59:22.057203 Epoch 209, Training loss 0.6895283567326148\n",
      "2022-12-10 22:59:35.927077 Epoch 210, Training loss 0.68900672836072\n",
      "2022-12-10 22:59:50.070272 Epoch 211, Training loss 0.6870536784381818\n",
      "2022-12-10 23:00:04.133047 Epoch 212, Training loss 0.6867022325315744\n",
      "2022-12-10 23:00:18.012173 Epoch 213, Training loss 0.6856337217876064\n",
      "2022-12-10 23:00:31.792056 Epoch 214, Training loss 0.6857734476513875\n",
      "2022-12-10 23:00:46.232162 Epoch 215, Training loss 0.6850339773365909\n",
      "2022-12-10 23:01:00.133910 Epoch 216, Training loss 0.6843288940237001\n",
      "2022-12-10 23:01:14.027885 Epoch 217, Training loss 0.6836991077646271\n",
      "2022-12-10 23:01:27.923022 Epoch 218, Training loss 0.681871434154413\n",
      "2022-12-10 23:01:41.620226 Epoch 219, Training loss 0.6821048550898462\n",
      "2022-12-10 23:01:55.675666 Epoch 220, Training loss 0.6803224321521456\n",
      "2022-12-10 23:02:09.229572 Epoch 221, Training loss 0.6802652377606658\n",
      "2022-12-10 23:02:23.046961 Epoch 222, Training loss 0.6782492110338967\n",
      "2022-12-10 23:02:37.687461 Epoch 223, Training loss 0.6789777907721527\n",
      "2022-12-10 23:02:52.344427 Epoch 224, Training loss 0.6788121246925706\n",
      "2022-12-10 23:03:06.962139 Epoch 225, Training loss 0.6766256733471171\n",
      "2022-12-10 23:03:21.564745 Epoch 226, Training loss 0.6757719601359209\n",
      "2022-12-10 23:03:36.149000 Epoch 227, Training loss 0.6756115162464054\n",
      "2022-12-10 23:03:50.766816 Epoch 228, Training loss 0.6737423574223238\n",
      "2022-12-10 23:04:05.332438 Epoch 229, Training loss 0.6730693526127759\n",
      "2022-12-10 23:04:19.970572 Epoch 230, Training loss 0.6726078962730935\n",
      "2022-12-10 23:04:34.619791 Epoch 231, Training loss 0.6716287450869675\n",
      "2022-12-10 23:04:49.248468 Epoch 232, Training loss 0.6719487860532063\n",
      "2022-12-10 23:05:03.896739 Epoch 233, Training loss 0.6706834235764525\n",
      "2022-12-10 23:05:18.533786 Epoch 234, Training loss 0.6702862658616527\n",
      "2022-12-10 23:05:33.256425 Epoch 235, Training loss 0.6691718655626487\n",
      "2022-12-10 23:05:47.876769 Epoch 236, Training loss 0.6682897704031766\n",
      "2022-12-10 23:06:02.457188 Epoch 237, Training loss 0.6677093265763939\n",
      "2022-12-10 23:06:17.046207 Epoch 238, Training loss 0.6669237804229912\n",
      "2022-12-10 23:06:31.660827 Epoch 239, Training loss 0.6660692339663006\n",
      "2022-12-10 23:06:46.213030 Epoch 240, Training loss 0.6657827897449894\n",
      "2022-12-10 23:07:00.883804 Epoch 241, Training loss 0.6654742477495043\n",
      "2022-12-10 23:07:15.493035 Epoch 242, Training loss 0.6640466221458162\n",
      "2022-12-10 23:07:30.093361 Epoch 243, Training loss 0.6627961670041389\n",
      "2022-12-10 23:07:44.709516 Epoch 244, Training loss 0.6635654195952598\n",
      "2022-12-10 23:07:59.398245 Epoch 245, Training loss 0.6615192024299251\n",
      "2022-12-10 23:08:14.052687 Epoch 246, Training loss 0.6616744274068671\n",
      "2022-12-10 23:08:28.700111 Epoch 247, Training loss 0.6605225793845818\n",
      "2022-12-10 23:08:43.282118 Epoch 248, Training loss 0.6602284593502884\n",
      "2022-12-10 23:08:57.894610 Epoch 249, Training loss 0.6588036681684997\n",
      "2022-12-10 23:09:12.456721 Epoch 250, Training loss 0.6586067916639625\n",
      "2022-12-10 23:09:27.097957 Epoch 251, Training loss 0.6585556879982619\n",
      "2022-12-10 23:09:41.795107 Epoch 252, Training loss 0.6568206291826789\n",
      "2022-12-10 23:09:56.424196 Epoch 253, Training loss 0.6565654516372534\n",
      "2022-12-10 23:10:11.202496 Epoch 254, Training loss 0.6549783874960506\n",
      "2022-12-10 23:10:25.835524 Epoch 255, Training loss 0.6554743117078796\n",
      "2022-12-10 23:10:40.457449 Epoch 256, Training loss 0.6555447916088202\n",
      "2022-12-10 23:10:55.130926 Epoch 257, Training loss 0.6539027623050963\n",
      "2022-12-10 23:11:09.802417 Epoch 258, Training loss 0.6541270266103623\n",
      "2022-12-10 23:11:24.386023 Epoch 259, Training loss 0.6528696201341536\n",
      "2022-12-10 23:11:38.889385 Epoch 260, Training loss 0.653119062447487\n",
      "2022-12-10 23:11:53.539925 Epoch 261, Training loss 0.6517294013439237\n",
      "2022-12-10 23:12:08.138447 Epoch 262, Training loss 0.6513704275688552\n",
      "2022-12-10 23:12:22.026950 Epoch 263, Training loss 0.6493396530370883\n",
      "2022-12-10 23:12:35.375409 Epoch 264, Training loss 0.6522415904590236\n",
      "2022-12-10 23:12:49.942013 Epoch 265, Training loss 0.647980559283815\n",
      "2022-12-10 23:13:04.520977 Epoch 266, Training loss 0.6469965836276179\n",
      "2022-12-10 23:13:19.199053 Epoch 267, Training loss 0.6481007570805757\n",
      "2022-12-10 23:13:33.855194 Epoch 268, Training loss 0.646680643796311\n",
      "2022-12-10 23:13:48.548445 Epoch 269, Training loss 0.645918352448422\n",
      "2022-12-10 23:14:03.099281 Epoch 270, Training loss 0.6457636912765405\n",
      "2022-12-10 23:14:17.739991 Epoch 271, Training loss 0.6443854652707229\n",
      "2022-12-10 23:14:32.330799 Epoch 272, Training loss 0.6449539545551896\n",
      "2022-12-10 23:14:47.009517 Epoch 273, Training loss 0.6425867019711858\n",
      "2022-12-10 23:15:01.652478 Epoch 274, Training loss 0.643020971428098\n",
      "2022-12-10 23:15:16.286480 Epoch 275, Training loss 0.6420470031783404\n",
      "2022-12-10 23:15:30.823706 Epoch 276, Training loss 0.6421989650677538\n",
      "2022-12-10 23:15:45.441937 Epoch 277, Training loss 0.6418880405633346\n",
      "2022-12-10 23:16:00.185613 Epoch 278, Training loss 0.640594169992925\n",
      "2022-12-10 23:16:14.841733 Epoch 279, Training loss 0.6412767434821409\n",
      "2022-12-10 23:16:29.558375 Epoch 280, Training loss 0.6393794025606512\n",
      "2022-12-10 23:16:44.280250 Epoch 281, Training loss 0.639505406551044\n",
      "2022-12-10 23:16:58.862458 Epoch 282, Training loss 0.6375005661374162\n",
      "2022-12-10 23:17:13.440349 Epoch 283, Training loss 0.6384724695664232\n",
      "2022-12-10 23:17:28.038559 Epoch 284, Training loss 0.6371633285451728\n",
      "2022-12-10 23:17:42.751951 Epoch 285, Training loss 0.6367492757337477\n",
      "2022-12-10 23:17:57.345280 Epoch 286, Training loss 0.6347204933081136\n",
      "2022-12-10 23:18:11.906822 Epoch 287, Training loss 0.636332292767132\n",
      "2022-12-10 23:18:26.462854 Epoch 288, Training loss 0.6348248538001419\n",
      "2022-12-10 23:18:41.023622 Epoch 289, Training loss 0.633772713525216\n",
      "2022-12-10 23:18:55.615315 Epoch 290, Training loss 0.635432627423645\n",
      "2022-12-10 23:19:10.186139 Epoch 291, Training loss 0.6328399515212955\n",
      "2022-12-10 23:19:24.815966 Epoch 292, Training loss 0.6308439541655733\n",
      "2022-12-10 23:19:39.379824 Epoch 293, Training loss 0.6309619110715968\n",
      "2022-12-10 23:19:53.954660 Epoch 294, Training loss 0.6317406920978176\n",
      "2022-12-10 23:20:08.775359 Epoch 295, Training loss 0.6310596101729157\n",
      "2022-12-10 23:20:23.390961 Epoch 296, Training loss 0.6304930619266637\n",
      "2022-12-10 23:20:38.010909 Epoch 297, Training loss 0.628794339306824\n",
      "2022-12-10 23:20:52.622748 Epoch 298, Training loss 0.6278755280672742\n",
      "2022-12-10 23:21:07.377463 Epoch 299, Training loss 0.6275831274211864\n",
      "2022-12-10 23:21:19.686524 Epoch 300, Training loss 0.628146682858772\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=128, shuffle=True)\n",
    "model_a = Net().to(device=device)\n",
    "optimizer = torch.optim.SGD(model_a.parameters(), lr=l_r)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(num_epochs, optimizer, model_a, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=128, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model_a(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "    print(\"Accuracy: {:.2f}\".format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(8, 3, kernel_size=3, padding=1)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_r = .02\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-10 23:28:30.487254 Epoch 1, Training loss 2.205159378295042\n",
      "2022-12-10 23:28:43.116394 Epoch 2, Training loss 1.94727153255015\n",
      "2022-12-10 23:28:55.956614 Epoch 3, Training loss 1.7649597884440908\n",
      "2022-12-10 23:29:07.961058 Epoch 4, Training loss 1.6282386335791375\n",
      "2022-12-10 23:29:20.687763 Epoch 5, Training loss 1.5353622661561381\n",
      "2022-12-10 23:29:33.986395 Epoch 6, Training loss 1.4670775411080341\n",
      "2022-12-10 23:29:46.892074 Epoch 7, Training loss 1.418908684837575\n",
      "2022-12-10 23:29:59.886462 Epoch 8, Training loss 1.372822064526227\n",
      "2022-12-10 23:30:12.591734 Epoch 9, Training loss 1.341478488274983\n",
      "2022-12-10 23:30:25.580109 Epoch 10, Training loss 1.3129363175557585\n",
      "2022-12-10 23:30:38.837057 Epoch 11, Training loss 1.2900027955064968\n",
      "2022-12-10 23:30:51.707133 Epoch 12, Training loss 1.2732123100027746\n",
      "2022-12-10 23:31:04.532012 Epoch 13, Training loss 1.25021501706571\n",
      "2022-12-10 23:31:17.716773 Epoch 14, Training loss 1.229579393352781\n",
      "2022-12-10 23:31:30.262882 Epoch 15, Training loss 1.214292559088493\n",
      "2022-12-10 23:31:43.449156 Epoch 16, Training loss 1.1970481039309988\n",
      "2022-12-10 23:31:56.062470 Epoch 17, Training loss 1.181207307443327\n",
      "2022-12-10 23:32:09.262160 Epoch 18, Training loss 1.1670677610197846\n",
      "2022-12-10 23:32:22.224671 Epoch 19, Training loss 1.1515479249005416\n",
      "2022-12-10 23:32:34.900147 Epoch 20, Training loss 1.1353487618723694\n",
      "2022-12-10 23:32:47.810782 Epoch 21, Training loss 1.1192903044272442\n",
      "2022-12-10 23:33:00.693749 Epoch 22, Training loss 1.1093837634033086\n",
      "2022-12-10 23:33:14.095633 Epoch 23, Training loss 1.0941158901063763\n",
      "2022-12-10 23:33:28.489336 Epoch 24, Training loss 1.0825683307282779\n",
      "2022-12-10 23:33:41.708646 Epoch 25, Training loss 1.0696152460818389\n",
      "2022-12-10 23:33:54.420820 Epoch 26, Training loss 1.0564993203294522\n",
      "2022-12-10 23:34:07.712775 Epoch 27, Training loss 1.0500065565717465\n",
      "2022-12-10 23:34:20.644690 Epoch 28, Training loss 1.0371297996263116\n",
      "2022-12-10 23:34:33.315353 Epoch 29, Training loss 1.0285388912473405\n",
      "2022-12-10 23:34:46.357415 Epoch 30, Training loss 1.0196762589775785\n",
      "2022-12-10 23:34:59.450963 Epoch 31, Training loss 1.006824708714777\n",
      "2022-12-10 23:35:13.601651 Epoch 32, Training loss 1.0015354287259433\n",
      "2022-12-10 23:35:27.772919 Epoch 33, Training loss 0.9962890157286002\n",
      "2022-12-10 23:35:41.880851 Epoch 34, Training loss 0.9822552575140583\n",
      "2022-12-10 23:35:55.988186 Epoch 35, Training loss 0.9783669853088807\n",
      "2022-12-10 23:36:10.165776 Epoch 36, Training loss 0.9719570957276286\n",
      "2022-12-10 23:36:24.406732 Epoch 37, Training loss 0.9622832041005699\n",
      "2022-12-10 23:36:38.503922 Epoch 38, Training loss 0.9552074190305204\n",
      "2022-12-10 23:36:52.610780 Epoch 39, Training loss 0.9528048853484952\n",
      "2022-12-10 23:37:06.761762 Epoch 40, Training loss 0.9445655130001963\n",
      "2022-12-10 23:37:20.888172 Epoch 41, Training loss 0.9429426293592064\n",
      "2022-12-10 23:37:34.570942 Epoch 42, Training loss 0.9329045427088835\n",
      "2022-12-10 23:37:48.758877 Epoch 43, Training loss 0.9262329537649544\n",
      "2022-12-10 23:38:02.908238 Epoch 44, Training loss 0.9199157743429651\n",
      "2022-12-10 23:38:17.032400 Epoch 45, Training loss 0.9125507860159388\n",
      "2022-12-10 23:38:31.160157 Epoch 46, Training loss 0.9159546339390229\n",
      "2022-12-10 23:38:45.302521 Epoch 47, Training loss 0.9110486203310441\n",
      "2022-12-10 23:38:59.721134 Epoch 48, Training loss 0.8997875686202731\n",
      "2022-12-10 23:39:14.354117 Epoch 49, Training loss 0.8979364834269699\n",
      "2022-12-10 23:39:28.946916 Epoch 50, Training loss 0.8962100677344264\n",
      "2022-12-10 23:39:43.357230 Epoch 51, Training loss 0.8919292362977047\n",
      "2022-12-10 23:39:57.533733 Epoch 52, Training loss 0.882931789573358\n",
      "2022-12-10 23:40:11.729452 Epoch 53, Training loss 0.8810523632837801\n",
      "2022-12-10 23:40:25.854841 Epoch 54, Training loss 0.8790995177565789\n",
      "2022-12-10 23:40:40.028793 Epoch 55, Training loss 0.8747753482692096\n",
      "2022-12-10 23:40:54.143160 Epoch 56, Training loss 0.8694398980967852\n",
      "2022-12-10 23:41:08.101801 Epoch 57, Training loss 0.8692807266298606\n",
      "2022-12-10 23:41:22.154434 Epoch 58, Training loss 0.8623959750545268\n",
      "2022-12-10 23:41:35.849708 Epoch 59, Training loss 0.8612193668983421\n",
      "2022-12-10 23:41:50.077579 Epoch 60, Training loss 0.8543012841623656\n",
      "2022-12-10 23:42:03.961384 Epoch 61, Training loss 0.8532556553884428\n",
      "2022-12-10 23:42:16.983876 Epoch 62, Training loss 0.8503773169858115\n",
      "2022-12-10 23:42:31.144330 Epoch 63, Training loss 0.8478427769578233\n",
      "2022-12-10 23:42:45.307928 Epoch 64, Training loss 0.8434525244698232\n",
      "2022-12-10 23:42:59.468264 Epoch 65, Training loss 0.8401345367334327\n",
      "2022-12-10 23:43:13.606862 Epoch 66, Training loss 0.8364529901621293\n",
      "2022-12-10 23:43:27.756214 Epoch 67, Training loss 0.8346239684187636\n",
      "2022-12-10 23:43:41.906681 Epoch 68, Training loss 0.8370103687047958\n",
      "2022-12-10 23:43:56.078844 Epoch 69, Training loss 0.8316684456504121\n",
      "2022-12-10 23:44:10.287250 Epoch 70, Training loss 0.8275097426103086\n",
      "2022-12-10 23:44:24.421135 Epoch 71, Training loss 0.8198220523036256\n",
      "2022-12-10 23:44:37.542988 Epoch 72, Training loss 0.8234941731302106\n",
      "2022-12-10 23:44:51.731323 Epoch 73, Training loss 0.8162537299248637\n",
      "2022-12-10 23:45:05.932805 Epoch 74, Training loss 0.8164079402174268\n",
      "2022-12-10 23:45:20.127554 Epoch 75, Training loss 0.813888415998342\n",
      "2022-12-10 23:45:34.270706 Epoch 76, Training loss 0.81204426988047\n",
      "2022-12-10 23:45:48.412233 Epoch 77, Training loss 0.8049203461530258\n",
      "2022-12-10 23:46:02.927094 Epoch 78, Training loss 0.8081319703131306\n",
      "2022-12-10 23:46:18.977726 Epoch 79, Training loss 0.8034139883761503\n",
      "2022-12-10 23:46:34.216481 Epoch 80, Training loss 0.8031210832449854\n",
      "2022-12-10 23:46:47.498388 Epoch 81, Training loss 0.7968862588916507\n",
      "2022-12-10 23:47:01.984612 Epoch 82, Training loss 0.7992509804209884\n",
      "2022-12-10 23:47:14.522697 Epoch 83, Training loss 0.7928555829792606\n",
      "2022-12-10 23:47:26.679707 Epoch 84, Training loss 0.7924722226298585\n",
      "2022-12-10 23:47:39.906443 Epoch 85, Training loss 0.7879494985147398\n",
      "2022-12-10 23:47:52.168725 Epoch 86, Training loss 0.7883185671300305\n",
      "2022-12-10 23:48:06.004160 Epoch 87, Training loss 0.7835754724789639\n",
      "2022-12-10 23:48:20.197814 Epoch 88, Training loss 0.7835988548337197\n",
      "2022-12-10 23:48:34.384836 Epoch 89, Training loss 0.7837363569712152\n",
      "2022-12-10 23:48:48.604692 Epoch 90, Training loss 0.7812847133193698\n",
      "2022-12-10 23:49:02.726117 Epoch 91, Training loss 0.7787026030068495\n",
      "2022-12-10 23:49:16.945164 Epoch 92, Training loss 0.7754957186324256\n",
      "2022-12-10 23:49:31.104501 Epoch 93, Training loss 0.7719600988285882\n",
      "2022-12-10 23:49:43.815264 Epoch 94, Training loss 0.7748255005904606\n",
      "2022-12-10 23:49:57.554650 Epoch 95, Training loss 0.7716880580600427\n",
      "2022-12-10 23:50:11.700852 Epoch 96, Training loss 0.7631484683679075\n",
      "2022-12-10 23:50:25.871082 Epoch 97, Training loss 0.7655591657575296\n",
      "2022-12-10 23:50:40.081059 Epoch 98, Training loss 0.7641357609203884\n",
      "2022-12-10 23:50:54.236240 Epoch 99, Training loss 0.7592466364101488\n",
      "2022-12-10 23:51:08.413974 Epoch 100, Training loss 0.760017274289715\n",
      "2022-12-10 23:51:22.572270 Epoch 101, Training loss 0.7609314006202075\n",
      "2022-12-10 23:51:36.825012 Epoch 102, Training loss 0.7555362618699366\n",
      "2022-12-10 23:51:50.929361 Epoch 103, Training loss 0.7559913284316355\n",
      "2022-12-10 23:52:05.155302 Epoch 104, Training loss 0.7515344978595266\n",
      "2022-12-10 23:52:19.369363 Epoch 105, Training loss 0.7530740700205978\n",
      "2022-12-10 23:52:33.534268 Epoch 106, Training loss 0.7464367887195276\n",
      "2022-12-10 23:52:47.685525 Epoch 107, Training loss 0.7512128757578986\n",
      "2022-12-10 23:53:01.336260 Epoch 108, Training loss 0.7458061150142125\n",
      "2022-12-10 23:53:15.568061 Epoch 109, Training loss 0.7439239353549724\n",
      "2022-12-10 23:53:29.785809 Epoch 110, Training loss 0.741088520203318\n",
      "2022-12-10 23:53:43.978333 Epoch 111, Training loss 0.7428923927399577\n",
      "2022-12-10 23:53:58.123648 Epoch 112, Training loss 0.7393265053325769\n",
      "2022-12-10 23:54:12.302241 Epoch 113, Training loss 0.7386879601648876\n",
      "2022-12-10 23:54:26.465911 Epoch 114, Training loss 0.7316366875050019\n",
      "2022-12-10 23:54:40.656252 Epoch 115, Training loss 0.7344748882614837\n",
      "2022-12-10 23:54:54.917571 Epoch 116, Training loss 0.734309014313075\n",
      "2022-12-10 23:55:09.084099 Epoch 117, Training loss 0.7292893641457265\n",
      "2022-12-10 23:55:23.221760 Epoch 118, Training loss 0.7288346202397833\n",
      "2022-12-10 23:55:37.376513 Epoch 119, Training loss 0.7258758058353346\n",
      "2022-12-10 23:55:51.471555 Epoch 120, Training loss 0.7264798569435976\n",
      "2022-12-10 23:56:05.630048 Epoch 121, Training loss 0.7244019660414481\n",
      "2022-12-10 23:56:19.818195 Epoch 122, Training loss 0.7203899886535139\n",
      "2022-12-10 23:56:33.972337 Epoch 123, Training loss 0.7225287951985184\n",
      "2022-12-10 23:56:48.253746 Epoch 124, Training loss 0.7183462819274591\n",
      "2022-12-10 23:57:02.781521 Epoch 125, Training loss 0.7223506433014967\n",
      "2022-12-10 23:57:17.240745 Epoch 126, Training loss 0.7148980829788714\n",
      "2022-12-10 23:57:31.378001 Epoch 127, Training loss 0.7149110606738499\n",
      "2022-12-10 23:57:45.553649 Epoch 128, Training loss 0.7121138855510828\n",
      "2022-12-10 23:57:59.750516 Epoch 129, Training loss 0.7083604217183833\n",
      "2022-12-10 23:58:13.952786 Epoch 130, Training loss 0.7166252117984149\n",
      "2022-12-10 23:58:28.148411 Epoch 131, Training loss 0.7139885224858109\n",
      "2022-12-10 23:58:42.317774 Epoch 132, Training loss 0.7097749564112449\n",
      "2022-12-10 23:58:56.461843 Epoch 133, Training loss 0.7072564834842876\n",
      "2022-12-10 23:59:10.649566 Epoch 134, Training loss 0.7030789097961114\n",
      "2022-12-10 23:59:24.813826 Epoch 135, Training loss 0.7024551246847425\n",
      "2022-12-10 23:59:39.058622 Epoch 136, Training loss 0.7022443143080692\n",
      "2022-12-10 23:59:53.228202 Epoch 137, Training loss 0.6992880185039676\n",
      "2022-12-11 00:00:09.837446 Epoch 138, Training loss 0.6988473002399717\n",
      "2022-12-11 00:00:24.171643 Epoch 139, Training loss 0.6962483093446615\n",
      "2022-12-11 00:00:38.330228 Epoch 140, Training loss 0.7013756246591101\n",
      "2022-12-11 00:00:52.600525 Epoch 141, Training loss 0.7016812310535081\n",
      "2022-12-11 00:01:06.689947 Epoch 142, Training loss 0.6964980482446904\n",
      "2022-12-11 00:01:20.908555 Epoch 143, Training loss 0.6919397328581128\n",
      "2022-12-11 00:01:35.089425 Epoch 144, Training loss 0.6896799979161243\n",
      "2022-12-11 00:01:49.193254 Epoch 145, Training loss 0.6900881011875308\n",
      "2022-12-11 00:02:03.429901 Epoch 146, Training loss 0.6904187111222014\n",
      "2022-12-11 00:02:17.618374 Epoch 147, Training loss 0.6913778172159681\n",
      "2022-12-11 00:02:31.840126 Epoch 148, Training loss 0.68638670414078\n",
      "2022-12-11 00:02:46.057128 Epoch 149, Training loss 0.6849235810187398\n",
      "2022-12-11 00:03:00.302891 Epoch 150, Training loss 0.6884678736024973\n",
      "2022-12-11 00:03:14.423323 Epoch 151, Training loss 0.6818079607827323\n",
      "2022-12-11 00:03:28.868206 Epoch 152, Training loss 0.6795556365835423\n",
      "2022-12-11 00:03:43.118558 Epoch 153, Training loss 0.6827409556325601\n",
      "2022-12-11 00:03:57.313253 Epoch 154, Training loss 0.6771111625189684\n",
      "2022-12-11 00:04:11.498621 Epoch 155, Training loss 0.6789267415903053\n",
      "2022-12-11 00:04:25.717342 Epoch 156, Training loss 0.6733260040684622\n",
      "2022-12-11 00:04:40.218243 Epoch 157, Training loss 0.6801888824117427\n",
      "2022-12-11 00:04:54.438535 Epoch 158, Training loss 0.6789737161324949\n",
      "2022-12-11 00:05:08.622692 Epoch 159, Training loss 0.6678805460735243\n",
      "2022-12-11 00:05:22.816147 Epoch 160, Training loss 0.6776017651570087\n",
      "2022-12-11 00:05:37.347559 Epoch 161, Training loss 0.670739052247028\n",
      "2022-12-11 00:05:51.140446 Epoch 162, Training loss 0.6683975137314018\n",
      "2022-12-11 00:06:05.300397 Epoch 163, Training loss 0.6679693101620188\n",
      "2022-12-11 00:06:19.397628 Epoch 164, Training loss 0.6721868989418964\n",
      "2022-12-11 00:06:33.831555 Epoch 165, Training loss 0.6678898684224304\n",
      "2022-12-11 00:06:49.303928 Epoch 166, Training loss 0.667887726912693\n",
      "2022-12-11 00:07:03.517037 Epoch 167, Training loss 0.6643779977243773\n",
      "2022-12-11 00:07:17.690886 Epoch 168, Training loss 0.6613779740065945\n",
      "2022-12-11 00:07:31.839037 Epoch 169, Training loss 0.6652508134744606\n",
      "2022-12-11 00:07:46.071092 Epoch 170, Training loss 0.6568251349488083\n",
      "2022-12-11 00:07:59.210028 Epoch 171, Training loss 0.6654252519412917\n",
      "2022-12-11 00:08:13.374278 Epoch 172, Training loss 0.6603935712150165\n",
      "2022-12-11 00:08:27.802760 Epoch 173, Training loss 0.6612318203765519\n",
      "2022-12-11 00:08:42.015167 Epoch 174, Training loss 0.6591894568837419\n",
      "2022-12-11 00:08:56.547277 Epoch 175, Training loss 0.6557459715677767\n",
      "2022-12-11 00:09:10.769610 Epoch 176, Training loss 0.6533787317421972\n",
      "2022-12-11 00:09:24.961807 Epoch 177, Training loss 0.6546349824995411\n",
      "2022-12-11 00:09:39.158447 Epoch 178, Training loss 0.6553580678847372\n",
      "2022-12-11 00:09:53.456769 Epoch 179, Training loss 0.6553683131933212\n",
      "2022-12-11 00:10:07.738276 Epoch 180, Training loss 0.6532847815934493\n",
      "2022-12-11 00:10:21.950041 Epoch 181, Training loss 0.6556627879641495\n",
      "2022-12-11 00:10:36.117684 Epoch 182, Training loss 0.6476083300551589\n",
      "2022-12-11 00:10:50.350539 Epoch 183, Training loss 0.6511275225452015\n",
      "2022-12-11 00:11:04.541231 Epoch 184, Training loss 0.6488501449324646\n",
      "2022-12-11 00:11:18.290863 Epoch 185, Training loss 0.6492661277250368\n",
      "2022-12-11 00:11:32.796697 Epoch 186, Training loss 0.647346882339643\n",
      "2022-12-11 00:11:46.982386 Epoch 187, Training loss 0.6426655009997134\n",
      "2022-12-11 00:12:01.181012 Epoch 188, Training loss 0.6405029810813009\n",
      "2022-12-11 00:12:15.334726 Epoch 189, Training loss 0.6410832374679799\n",
      "2022-12-11 00:12:29.434212 Epoch 190, Training loss 0.6363610284669059\n",
      "2022-12-11 00:12:43.678785 Epoch 191, Training loss 0.6441464507762267\n",
      "2022-12-11 00:12:57.904766 Epoch 192, Training loss 0.6436122938686487\n",
      "2022-12-11 00:13:12.089915 Epoch 193, Training loss 0.6387604149628658\n",
      "2022-12-11 00:13:26.312572 Epoch 194, Training loss 0.631826292191233\n",
      "2022-12-11 00:13:40.694716 Epoch 195, Training loss 0.640612714752859\n",
      "2022-12-11 00:13:55.766949 Epoch 196, Training loss 0.6406845914161935\n",
      "2022-12-11 00:14:10.183882 Epoch 197, Training loss 0.6350021100774104\n",
      "2022-12-11 00:14:24.352784 Epoch 198, Training loss 0.6362076938456419\n",
      "2022-12-11 00:14:38.751756 Epoch 199, Training loss 0.6331460821081181\n",
      "2022-12-11 00:14:54.019871 Epoch 200, Training loss 0.6353411133192024\n",
      "2022-12-11 00:15:08.182187 Epoch 201, Training loss 0.6387182090963636\n",
      "2022-12-11 00:15:22.299994 Epoch 202, Training loss 0.6241939492067512\n",
      "2022-12-11 00:15:36.034409 Epoch 203, Training loss 0.6295168871174053\n",
      "2022-12-11 00:15:50.543222 Epoch 204, Training loss 0.6227209466148396\n",
      "2022-12-11 00:16:04.665867 Epoch 205, Training loss 0.6198060252532667\n",
      "2022-12-11 00:16:18.827204 Epoch 206, Training loss 0.626884501199333\n",
      "2022-12-11 00:16:33.056501 Epoch 207, Training loss 0.633327879467789\n",
      "2022-12-11 00:16:47.504389 Epoch 208, Training loss 0.633589370366262\n",
      "2022-12-11 00:17:01.658621 Epoch 209, Training loss 0.6235390139781699\n",
      "2022-12-11 00:17:15.894662 Epoch 210, Training loss 0.6234381749313704\n",
      "2022-12-11 00:17:30.067061 Epoch 211, Training loss 0.6144816729487205\n",
      "2022-12-11 00:17:44.272358 Epoch 212, Training loss 0.6251464293015246\n",
      "2022-12-11 00:18:00.112891 Epoch 213, Training loss 0.6300130567076255\n",
      "2022-12-11 00:18:14.573359 Epoch 214, Training loss 0.6202961071109285\n",
      "2022-12-11 00:18:28.724324 Epoch 215, Training loss 0.6183950321710839\n",
      "2022-12-11 00:18:43.217150 Epoch 216, Training loss 0.6185270686234746\n",
      "2022-12-11 00:18:57.272350 Epoch 217, Training loss 0.6230066438414612\n",
      "2022-12-11 00:19:12.265211 Epoch 218, Training loss 0.6150289177894592\n",
      "2022-12-11 00:19:26.800792 Epoch 219, Training loss 0.6182372310027784\n",
      "2022-12-11 00:19:41.007792 Epoch 220, Training loss 0.6196886381628562\n",
      "2022-12-11 00:19:57.762101 Epoch 221, Training loss 0.624848202022971\n",
      "2022-12-11 00:20:13.410388 Epoch 222, Training loss 0.6134160094115199\n",
      "2022-12-11 00:20:27.594717 Epoch 223, Training loss 0.6118449544420048\n",
      "2022-12-11 00:20:42.318682 Epoch 224, Training loss 0.6157618754980515\n",
      "2022-12-11 00:20:55.615998 Epoch 225, Training loss 0.6139838354928153\n",
      "2022-12-11 00:21:10.241606 Epoch 226, Training loss 0.6089359330279487\n",
      "2022-12-11 00:21:24.363992 Epoch 227, Training loss 0.6229357334728144\n",
      "2022-12-11 00:21:39.286688 Epoch 228, Training loss 0.615403068613033\n",
      "2022-12-11 00:21:53.507793 Epoch 229, Training loss 0.6083663613820562\n",
      "2022-12-11 00:22:07.792561 Epoch 230, Training loss 0.6133093271328478\n",
      "2022-12-11 00:22:22.002040 Epoch 231, Training loss 0.6020176565768768\n",
      "2022-12-11 00:22:35.959625 Epoch 232, Training loss 0.6010426588508547\n",
      "2022-12-11 00:22:51.478841 Epoch 233, Training loss 0.6093995673011761\n",
      "2022-12-11 00:23:05.668649 Epoch 234, Training loss 0.6101833022370631\n",
      "2022-12-11 00:23:19.773997 Epoch 235, Training loss 0.6022156382701835\n",
      "2022-12-11 00:23:34.259445 Epoch 236, Training loss 0.6053249068102058\n",
      "2022-12-11 00:23:48.776693 Epoch 237, Training loss 0.6091214226824897\n",
      "2022-12-11 00:24:02.407612 Epoch 238, Training loss 0.6007229591510734\n",
      "2022-12-11 00:24:16.517797 Epoch 239, Training loss 0.598563343894725\n",
      "2022-12-11 00:24:30.699091 Epoch 240, Training loss 0.6048372653978211\n",
      "2022-12-11 00:24:44.532761 Epoch 241, Training loss 0.6042099128268204\n",
      "2022-12-11 00:25:00.508233 Epoch 242, Training loss 0.5993566794358954\n",
      "2022-12-11 00:25:15.012463 Epoch 243, Training loss 0.6064949870413664\n",
      "2022-12-11 00:25:29.208331 Epoch 244, Training loss 0.6005517226092669\n",
      "2022-12-11 00:25:43.765595 Epoch 245, Training loss 0.6010504819604815\n",
      "2022-12-11 00:25:57.895031 Epoch 246, Training loss 0.5993740038604153\n",
      "2022-12-11 00:26:13.879354 Epoch 247, Training loss 0.6039092271607749\n",
      "2022-12-11 00:26:28.415234 Epoch 248, Training loss 0.5962866687653016\n",
      "2022-12-11 00:26:42.602682 Epoch 249, Training loss 0.590920520376186\n",
      "2022-12-11 00:26:56.823966 Epoch 250, Training loss 0.5954471416011149\n",
      "2022-12-11 00:27:11.921405 Epoch 251, Training loss 0.5921725049919012\n",
      "2022-12-11 00:27:25.942356 Epoch 252, Training loss 0.5956609728080886\n",
      "2022-12-11 00:27:40.082241 Epoch 253, Training loss 0.5977093209417499\n",
      "2022-12-11 00:27:54.231049 Epoch 254, Training loss 0.5980355485665555\n",
      "2022-12-11 00:28:08.349233 Epoch 255, Training loss 0.5960130816211506\n",
      "2022-12-11 00:28:22.493697 Epoch 256, Training loss 0.5925329575429157\n",
      "2022-12-11 00:28:37.540696 Epoch 257, Training loss 0.593552079279812\n",
      "2022-12-11 00:28:51.719186 Epoch 258, Training loss 0.5946472191384861\n",
      "2022-12-11 00:29:06.229699 Epoch 259, Training loss 0.5835877331543942\n",
      "2022-12-11 00:29:21.305926 Epoch 260, Training loss 0.5915181258199166\n",
      "2022-12-11 00:29:35.822437 Epoch 261, Training loss 0.5910021013447216\n",
      "2022-12-11 00:29:51.649966 Epoch 262, Training loss 0.587473527053181\n",
      "2022-12-11 00:30:07.194890 Epoch 263, Training loss 0.5850136001803437\n",
      "2022-12-11 00:30:21.290751 Epoch 264, Training loss 0.5920443767491652\n",
      "2022-12-11 00:30:34.978305 Epoch 265, Training loss 0.583146911038428\n",
      "2022-12-11 00:30:49.195139 Epoch 266, Training loss 0.5865333217139147\n",
      "2022-12-11 00:31:02.118189 Epoch 267, Training loss 0.5840000510215759\n",
      "2022-12-11 00:31:15.738115 Epoch 268, Training loss 0.5861984283036116\n",
      "2022-12-11 00:31:28.653386 Epoch 269, Training loss 0.5905802918940174\n",
      "2022-12-11 00:31:43.513553 Epoch 270, Training loss 0.5791118117619534\n",
      "2022-12-11 00:31:56.538855 Epoch 271, Training loss 0.5856665197987946\n",
      "2022-12-11 00:32:09.504694 Epoch 272, Training loss 0.575240604427396\n",
      "2022-12-11 00:32:23.731377 Epoch 273, Training loss 0.5907246482615568\n",
      "2022-12-11 00:32:55.071264 Epoch 274, Training loss 0.5793508394640319\n",
      "2022-12-11 00:33:20.285866 Epoch 275, Training loss 0.572489816771478\n",
      "2022-12-11 00:33:35.121487 Epoch 276, Training loss 0.5778414334873764\n",
      "2022-12-11 00:33:51.306825 Epoch 277, Training loss 0.5821024907790885\n",
      "2022-12-11 00:34:06.780357 Epoch 278, Training loss 0.5796354229048807\n",
      "2022-12-11 00:34:24.906475 Epoch 279, Training loss 0.5817050143164031\n",
      "2022-12-11 00:34:40.464136 Epoch 280, Training loss 0.5756776848313759\n",
      "2022-12-11 00:34:55.813119 Epoch 281, Training loss 0.5779920150430835\n",
      "2022-12-11 00:35:12.772880 Epoch 282, Training loss 0.5875371282502097\n",
      "2022-12-11 00:35:29.041319 Epoch 283, Training loss 0.5786030858146901\n",
      "2022-12-11 00:35:51.479668 Epoch 284, Training loss 0.5778304528521032\n",
      "2022-12-11 00:36:05.027217 Epoch 285, Training loss 0.5701571470316575\n",
      "2022-12-11 00:36:20.927332 Epoch 286, Training loss 0.5769415949376262\n",
      "2022-12-11 00:36:36.063796 Epoch 287, Training loss 0.5758710430592907\n",
      "2022-12-11 00:36:55.625859 Epoch 288, Training loss 0.5690248111376957\n",
      "2022-12-11 00:37:11.044714 Epoch 289, Training loss 0.5824075196774638\n",
      "2022-12-11 00:37:25.105408 Epoch 290, Training loss 0.5784515740007771\n",
      "2022-12-11 00:37:39.759370 Epoch 291, Training loss 0.5664470004183906\n",
      "2022-12-11 00:37:55.009538 Epoch 292, Training loss 0.5679426948939051\n",
      "2022-12-11 00:38:11.659124 Epoch 293, Training loss 0.5717022517810062\n",
      "2022-12-11 00:38:38.491045 Epoch 294, Training loss 0.5768400655717266\n",
      "2022-12-11 00:38:58.434442 Epoch 295, Training loss 0.5703178674590831\n",
      "2022-12-11 00:39:13.792698 Epoch 296, Training loss 0.5654620286457392\n",
      "2022-12-11 00:39:28.505030 Epoch 297, Training loss 0.5657793969220045\n",
      "2022-12-11 00:39:42.004821 Epoch 298, Training loss 0.5733760685032728\n",
      "2022-12-11 00:39:56.182054 Epoch 299, Training loss 0.5674857101878341\n",
      "2022-12-11 00:40:09.505616 Epoch 300, Training loss 0.5656508883955528\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=256, shuffle=True)\n",
    "model_b = Net().to(device=device)\n",
    "optimizer = torch.optim.SGD(model_b.parameters(), lr=l_r)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(num_epochs, optimizer, model_b, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.60\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=256, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model_b(imgs)\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "    print(\"Accuracy: {:.2f}\".format(correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0eb5d0a65b500759bcde1c4c1ad0551eaece71d5bef76353acf57400c52edb49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
