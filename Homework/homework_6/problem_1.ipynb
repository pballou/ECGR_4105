{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPatrick Ballou\\nID: 801130521\\nECGR 4105\\nHomework 6\\nProblem 1\\n'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Patrick Ballou\n",
    "ID: 801130521\n",
    "ECGR 4105\n",
    "Homework 6\n",
    "Problem 1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = 'C:/Users/patri/Documents/School_Fall_2022/ECGR_4105/Homework/homework_6'\n",
    "#normalized\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True, transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hidden layer\n",
    "model_a = nn.Sequential(\n",
    "            nn.Linear(3072,512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro T2000'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3072, out_features=512, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move to gpu\n",
    "model_a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_r = .002\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_a.parameters(), lr=l_r)\n",
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.830380\n",
      "Epoch: 1, Loss: 1.975224\n",
      "Epoch: 2, Loss: 1.849329\n",
      "Epoch: 3, Loss: 1.843616\n",
      "Epoch: 4, Loss: 1.878878\n",
      "Epoch: 5, Loss: 1.876666\n",
      "Epoch: 6, Loss: 1.866058\n",
      "Epoch: 7, Loss: 1.834244\n",
      "Epoch: 8, Loss: 1.814626\n",
      "Epoch: 9, Loss: 1.717482\n",
      "Epoch: 10, Loss: 1.749159\n",
      "Epoch: 11, Loss: 1.638357\n",
      "Epoch: 12, Loss: 1.934950\n",
      "Epoch: 13, Loss: 1.749440\n",
      "Epoch: 14, Loss: 2.029092\n",
      "Epoch: 15, Loss: 1.969803\n",
      "Epoch: 16, Loss: 1.946956\n",
      "Epoch: 17, Loss: 1.817966\n",
      "Epoch: 18, Loss: 1.797036\n",
      "Epoch: 19, Loss: 1.628987\n",
      "Epoch: 20, Loss: 1.668926\n",
      "Epoch: 21, Loss: 1.761377\n",
      "Epoch: 22, Loss: 1.705829\n",
      "Epoch: 23, Loss: 1.857571\n",
      "Epoch: 24, Loss: 2.020938\n",
      "Epoch: 25, Loss: 1.587944\n",
      "Epoch: 26, Loss: 1.708810\n",
      "Epoch: 27, Loss: 1.735573\n",
      "Epoch: 28, Loss: 1.720546\n",
      "Epoch: 29, Loss: 1.694691\n",
      "Epoch: 30, Loss: 1.891228\n",
      "Epoch: 31, Loss: 1.657503\n",
      "Epoch: 32, Loss: 1.614375\n",
      "Epoch: 33, Loss: 1.755880\n",
      "Epoch: 34, Loss: 1.487533\n",
      "Epoch: 35, Loss: 1.591486\n",
      "Epoch: 36, Loss: 1.649389\n",
      "Epoch: 37, Loss: 1.710340\n",
      "Epoch: 38, Loss: 1.841892\n",
      "Epoch: 39, Loss: 1.474391\n",
      "Epoch: 40, Loss: 1.836172\n",
      "Epoch: 41, Loss: 1.810567\n",
      "Epoch: 42, Loss: 1.689059\n",
      "Epoch: 43, Loss: 1.585100\n",
      "Epoch: 44, Loss: 1.540840\n",
      "Epoch: 45, Loss: 1.783811\n",
      "Epoch: 46, Loss: 1.461412\n",
      "Epoch: 47, Loss: 1.756535\n",
      "Epoch: 48, Loss: 1.847187\n",
      "Epoch: 49, Loss: 2.014337\n",
      "Epoch: 50, Loss: 1.710624\n",
      "Epoch: 51, Loss: 1.661636\n",
      "Epoch: 52, Loss: 1.626320\n",
      "Epoch: 53, Loss: 1.805043\n",
      "Epoch: 54, Loss: 1.601557\n",
      "Epoch: 55, Loss: 1.792169\n",
      "Epoch: 56, Loss: 1.482822\n",
      "Epoch: 57, Loss: 1.533759\n",
      "Epoch: 58, Loss: 1.710453\n",
      "Epoch: 59, Loss: 1.517948\n",
      "Epoch: 60, Loss: 1.589384\n",
      "Epoch: 61, Loss: 1.453722\n",
      "Epoch: 62, Loss: 1.651487\n",
      "Epoch: 63, Loss: 1.420582\n",
      "Epoch: 64, Loss: 1.565559\n",
      "Epoch: 65, Loss: 1.851331\n",
      "Epoch: 66, Loss: 1.664756\n",
      "Epoch: 67, Loss: 1.587819\n",
      "Epoch: 68, Loss: 1.649750\n",
      "Epoch: 69, Loss: 1.569828\n",
      "Epoch: 70, Loss: 1.529196\n",
      "Epoch: 71, Loss: 1.525078\n",
      "Epoch: 72, Loss: 1.590867\n",
      "Epoch: 73, Loss: 1.812189\n",
      "Epoch: 74, Loss: 1.212054\n",
      "Epoch: 75, Loss: 1.403548\n",
      "Epoch: 76, Loss: 1.497714\n",
      "Epoch: 77, Loss: 1.466328\n",
      "Epoch: 78, Loss: 1.587299\n",
      "Epoch: 79, Loss: 1.372588\n",
      "Epoch: 80, Loss: 1.436152\n",
      "Epoch: 81, Loss: 1.630036\n",
      "Epoch: 82, Loss: 1.439608\n",
      "Epoch: 83, Loss: 1.686939\n",
      "Epoch: 84, Loss: 1.916663\n",
      "Epoch: 85, Loss: 1.497437\n",
      "Epoch: 86, Loss: 1.592362\n",
      "Epoch: 87, Loss: 1.604804\n",
      "Epoch: 88, Loss: 1.606800\n",
      "Epoch: 89, Loss: 1.270441\n",
      "Epoch: 90, Loss: 1.498697\n",
      "Epoch: 91, Loss: 1.413336\n",
      "Epoch: 92, Loss: 1.359442\n",
      "Epoch: 93, Loss: 1.550365\n",
      "Epoch: 94, Loss: 1.251211\n",
      "Epoch: 95, Loss: 1.502292\n",
      "Epoch: 96, Loss: 1.659042\n",
      "Epoch: 97, Loss: 1.381871\n",
      "Epoch: 98, Loss: 1.409628\n",
      "Epoch: 99, Loss: 1.496023\n",
      "Epoch: 100, Loss: 1.243275\n",
      "Epoch: 101, Loss: 1.518440\n",
      "Epoch: 102, Loss: 1.741562\n",
      "Epoch: 103, Loss: 1.626041\n",
      "Epoch: 104, Loss: 1.398134\n",
      "Epoch: 105, Loss: 1.440250\n",
      "Epoch: 106, Loss: 1.463718\n",
      "Epoch: 107, Loss: 1.488112\n",
      "Epoch: 108, Loss: 1.621097\n",
      "Epoch: 109, Loss: 1.446499\n",
      "Epoch: 110, Loss: 1.612020\n",
      "Epoch: 111, Loss: 1.731316\n",
      "Epoch: 112, Loss: 1.391259\n",
      "Epoch: 113, Loss: 1.398203\n",
      "Epoch: 114, Loss: 1.531208\n",
      "Epoch: 115, Loss: 1.595702\n",
      "Epoch: 116, Loss: 1.628711\n",
      "Epoch: 117, Loss: 1.633103\n",
      "Epoch: 118, Loss: 1.281495\n",
      "Epoch: 119, Loss: 1.420043\n",
      "Epoch: 120, Loss: 1.642820\n",
      "Epoch: 121, Loss: 1.517546\n",
      "Epoch: 122, Loss: 1.229197\n",
      "Epoch: 123, Loss: 1.455593\n",
      "Epoch: 124, Loss: 1.590193\n",
      "Epoch: 125, Loss: 1.287894\n",
      "Epoch: 126, Loss: 1.420781\n",
      "Epoch: 127, Loss: 1.471048\n",
      "Epoch: 128, Loss: 1.406068\n",
      "Epoch: 129, Loss: 1.402471\n",
      "Epoch: 130, Loss: 1.556539\n",
      "Epoch: 131, Loss: 1.317857\n",
      "Epoch: 132, Loss: 1.384552\n",
      "Epoch: 133, Loss: 1.362206\n",
      "Epoch: 134, Loss: 1.371155\n",
      "Epoch: 135, Loss: 1.329015\n",
      "Epoch: 136, Loss: 1.405928\n",
      "Epoch: 137, Loss: 1.237124\n",
      "Epoch: 138, Loss: 1.340280\n",
      "Epoch: 139, Loss: 1.633500\n",
      "Epoch: 140, Loss: 1.295563\n",
      "Epoch: 141, Loss: 1.263401\n",
      "Epoch: 142, Loss: 1.357279\n",
      "Epoch: 143, Loss: 1.426428\n",
      "Epoch: 144, Loss: 1.474170\n",
      "Epoch: 145, Loss: 1.012608\n",
      "Epoch: 146, Loss: 1.402853\n",
      "Epoch: 147, Loss: 1.436497\n",
      "Epoch: 148, Loss: 1.280151\n",
      "Epoch: 149, Loss: 1.226379\n",
      "Epoch: 150, Loss: 1.307105\n",
      "Epoch: 151, Loss: 1.437686\n",
      "Epoch: 152, Loss: 1.393417\n",
      "Epoch: 153, Loss: 1.433767\n",
      "Epoch: 154, Loss: 1.510463\n",
      "Epoch: 155, Loss: 1.458329\n",
      "Epoch: 156, Loss: 1.198451\n",
      "Epoch: 157, Loss: 1.270291\n",
      "Epoch: 158, Loss: 1.203069\n",
      "Epoch: 159, Loss: 1.629848\n",
      "Epoch: 160, Loss: 1.417962\n",
      "Epoch: 161, Loss: 1.405342\n",
      "Epoch: 162, Loss: 1.324113\n",
      "Epoch: 163, Loss: 1.363252\n",
      "Epoch: 164, Loss: 1.414532\n",
      "Epoch: 165, Loss: 1.360867\n",
      "Epoch: 166, Loss: 1.644938\n",
      "Epoch: 167, Loss: 1.574692\n",
      "Epoch: 168, Loss: 1.254041\n",
      "Epoch: 169, Loss: 1.228406\n",
      "Epoch: 170, Loss: 1.371582\n",
      "Epoch: 171, Loss: 1.358553\n",
      "Epoch: 172, Loss: 1.253830\n",
      "Epoch: 173, Loss: 1.107036\n",
      "Epoch: 174, Loss: 1.260457\n",
      "Epoch: 175, Loss: 1.209090\n",
      "Epoch: 176, Loss: 1.355409\n",
      "Epoch: 177, Loss: 1.137098\n",
      "Epoch: 178, Loss: 1.187573\n",
      "Epoch: 179, Loss: 1.458739\n",
      "Epoch: 180, Loss: 1.133879\n",
      "Epoch: 181, Loss: 1.421390\n",
      "Epoch: 182, Loss: 1.337532\n",
      "Epoch: 183, Loss: 1.275644\n",
      "Epoch: 184, Loss: 1.470158\n",
      "Epoch: 185, Loss: 1.606486\n",
      "Epoch: 186, Loss: 1.041858\n",
      "Epoch: 187, Loss: 1.263257\n",
      "Epoch: 188, Loss: 1.279440\n",
      "Epoch: 189, Loss: 1.194176\n",
      "Epoch: 190, Loss: 1.407727\n",
      "Epoch: 191, Loss: 1.123659\n",
      "Epoch: 192, Loss: 1.101905\n",
      "Epoch: 193, Loss: 1.173969\n",
      "Epoch: 194, Loss: 1.209534\n",
      "Epoch: 195, Loss: 1.381774\n",
      "Epoch: 196, Loss: 1.266635\n",
      "Epoch: 197, Loss: 1.242110\n",
      "Epoch: 198, Loss: 1.147398\n",
      "Epoch: 199, Loss: 1.297130\n",
      "Epoch: 200, Loss: 1.144265\n",
      "Epoch: 201, Loss: 1.264098\n",
      "Epoch: 202, Loss: 1.269959\n",
      "Epoch: 203, Loss: 1.284696\n",
      "Epoch: 204, Loss: 1.156545\n",
      "Epoch: 205, Loss: 1.299049\n",
      "Epoch: 206, Loss: 1.038579\n",
      "Epoch: 207, Loss: 1.121245\n",
      "Epoch: 208, Loss: 1.165666\n",
      "Epoch: 209, Loss: 1.237504\n",
      "Epoch: 210, Loss: 1.280493\n",
      "Epoch: 211, Loss: 1.209827\n",
      "Epoch: 212, Loss: 1.174733\n",
      "Epoch: 213, Loss: 1.339459\n",
      "Epoch: 214, Loss: 1.328829\n",
      "Epoch: 215, Loss: 1.336193\n",
      "Epoch: 216, Loss: 1.325338\n",
      "Epoch: 217, Loss: 1.165435\n",
      "Epoch: 218, Loss: 1.246009\n",
      "Epoch: 219, Loss: 1.342925\n",
      "Epoch: 220, Loss: 1.336461\n",
      "Epoch: 221, Loss: 1.050348\n",
      "Epoch: 222, Loss: 1.248122\n",
      "Epoch: 223, Loss: 1.128356\n",
      "Epoch: 224, Loss: 1.229037\n",
      "Epoch: 225, Loss: 1.225406\n",
      "Epoch: 226, Loss: 1.191774\n",
      "Epoch: 227, Loss: 1.187903\n",
      "Epoch: 228, Loss: 1.209955\n",
      "Epoch: 229, Loss: 1.178248\n",
      "Epoch: 230, Loss: 1.247566\n",
      "Epoch: 231, Loss: 1.127708\n",
      "Epoch: 232, Loss: 1.300509\n",
      "Epoch: 233, Loss: 1.099195\n",
      "Epoch: 234, Loss: 1.257511\n",
      "Epoch: 235, Loss: 1.217770\n",
      "Epoch: 236, Loss: 1.040972\n",
      "Epoch: 237, Loss: 1.257901\n",
      "Epoch: 238, Loss: 1.294452\n",
      "Epoch: 239, Loss: 1.195047\n",
      "Epoch: 240, Loss: 1.174431\n",
      "Epoch: 241, Loss: 1.214147\n",
      "Epoch: 242, Loss: 1.288073\n",
      "Epoch: 243, Loss: 1.042904\n",
      "Epoch: 244, Loss: 1.004369\n",
      "Epoch: 245, Loss: 1.049875\n",
      "Epoch: 246, Loss: 1.198838\n",
      "Epoch: 247, Loss: 1.193267\n",
      "Epoch: 248, Loss: 1.377081\n",
      "Epoch: 249, Loss: 1.200194\n",
      "Epoch: 250, Loss: 1.187020\n",
      "Epoch: 251, Loss: 1.005733\n",
      "Epoch: 252, Loss: 1.332182\n",
      "Epoch: 253, Loss: 1.044926\n",
      "Epoch: 254, Loss: 0.949741\n",
      "Epoch: 255, Loss: 1.214462\n",
      "Epoch: 256, Loss: 1.227793\n",
      "Epoch: 257, Loss: 1.412740\n",
      "Epoch: 258, Loss: 1.064264\n",
      "Epoch: 259, Loss: 1.350299\n",
      "Epoch: 260, Loss: 1.076716\n",
      "Epoch: 261, Loss: 1.057215\n",
      "Epoch: 262, Loss: 1.149963\n",
      "Epoch: 263, Loss: 1.048952\n",
      "Epoch: 264, Loss: 1.075009\n",
      "Epoch: 265, Loss: 1.281075\n",
      "Epoch: 266, Loss: 1.061598\n",
      "Epoch: 267, Loss: 1.380394\n",
      "Epoch: 268, Loss: 0.994005\n",
      "Epoch: 269, Loss: 1.280600\n",
      "Epoch: 270, Loss: 0.971542\n",
      "Epoch: 271, Loss: 1.159168\n",
      "Epoch: 272, Loss: 1.299902\n",
      "Epoch: 273, Loss: 1.052519\n",
      "Epoch: 274, Loss: 1.113684\n",
      "Epoch: 275, Loss: 0.966753\n",
      "Epoch: 276, Loss: 0.941142\n",
      "Epoch: 277, Loss: 1.035878\n",
      "Epoch: 278, Loss: 1.011306\n",
      "Epoch: 279, Loss: 1.020775\n",
      "Epoch: 280, Loss: 1.110091\n",
      "Epoch: 281, Loss: 1.096480\n",
      "Epoch: 282, Loss: 1.016505\n",
      "Epoch: 283, Loss: 1.010025\n",
      "Epoch: 284, Loss: 1.239608\n",
      "Epoch: 285, Loss: 1.381129\n",
      "Epoch: 286, Loss: 1.041666\n",
      "Epoch: 287, Loss: 1.222587\n",
      "Epoch: 288, Loss: 1.140830\n",
      "Epoch: 289, Loss: 1.011175\n",
      "Epoch: 290, Loss: 0.990518\n",
      "Epoch: 291, Loss: 1.042813\n",
      "Epoch: 292, Loss: 1.062636\n",
      "Epoch: 293, Loss: 1.141650\n",
      "Epoch: 294, Loss: 1.178998\n",
      "Epoch: 295, Loss: 1.013862\n",
      "Epoch: 296, Loss: 1.138582\n",
      "Epoch: 297, Loss: 1.406964\n",
      "Epoch: 298, Loss: 1.016502\n",
      "Epoch: 299, Loss: 1.147049\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model_a(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model_a(imgs.view(batch_size, -1))\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "    print(\"Accuracy: {:.2f}\".format(correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#three hidden layers\n",
    "model_b = nn.Sequential(\n",
    "            nn.Linear(3072,1028),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(1028, 256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3072, out_features=1028, bias=True)\n",
       "  (1): LeakyReLU(negative_slope=0.01)\n",
       "  (2): Linear(in_features=1028, out_features=256, bias=True)\n",
       "  (3): LeakyReLU(negative_slope=0.01)\n",
       "  (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (5): LeakyReLU(negative_slope=0.01)\n",
       "  (6): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (7): LeakyReLU(negative_slope=0.01)\n",
       "  (8): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_b.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_r = .002\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_b.parameters(),lr=l_r)\n",
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle=True)\n",
    "loss_fn = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.285945\n",
      "Epoch: 1, Loss: 2.135777\n",
      "Epoch: 2, Loss: 2.190054\n",
      "Epoch: 3, Loss: 2.230818\n",
      "Epoch: 4, Loss: 1.935544\n",
      "Epoch: 5, Loss: 2.305283\n",
      "Epoch: 6, Loss: 2.045128\n",
      "Epoch: 7, Loss: 1.467916\n",
      "Epoch: 8, Loss: 2.027364\n",
      "Epoch: 9, Loss: 1.493638\n",
      "Epoch: 10, Loss: 1.303160\n",
      "Epoch: 11, Loss: 1.815007\n",
      "Epoch: 12, Loss: 1.256967\n",
      "Epoch: 13, Loss: 1.604033\n",
      "Epoch: 14, Loss: 1.311588\n",
      "Epoch: 15, Loss: 1.388225\n",
      "Epoch: 16, Loss: 1.227933\n",
      "Epoch: 17, Loss: 1.781184\n",
      "Epoch: 18, Loss: 1.533697\n",
      "Epoch: 19, Loss: 1.347286\n",
      "Epoch: 20, Loss: 1.151213\n",
      "Epoch: 21, Loss: 1.406174\n",
      "Epoch: 22, Loss: 1.322008\n",
      "Epoch: 23, Loss: 1.036471\n",
      "Epoch: 24, Loss: 1.042701\n",
      "Epoch: 25, Loss: 1.200212\n",
      "Epoch: 26, Loss: 1.334431\n",
      "Epoch: 27, Loss: 1.262972\n",
      "Epoch: 28, Loss: 0.866911\n",
      "Epoch: 29, Loss: 1.512697\n",
      "Epoch: 30, Loss: 1.009436\n",
      "Epoch: 31, Loss: 1.081901\n",
      "Epoch: 32, Loss: 1.071949\n",
      "Epoch: 33, Loss: 0.969334\n",
      "Epoch: 34, Loss: 0.844799\n",
      "Epoch: 35, Loss: 1.138213\n",
      "Epoch: 36, Loss: 0.860988\n",
      "Epoch: 37, Loss: 0.821808\n",
      "Epoch: 38, Loss: 1.159904\n",
      "Epoch: 39, Loss: 0.896800\n",
      "Epoch: 40, Loss: 0.953415\n",
      "Epoch: 41, Loss: 1.144317\n",
      "Epoch: 42, Loss: 0.909019\n",
      "Epoch: 43, Loss: 0.881377\n",
      "Epoch: 44, Loss: 0.689440\n",
      "Epoch: 45, Loss: 1.110556\n",
      "Epoch: 46, Loss: 2.073138\n",
      "Epoch: 47, Loss: 0.921748\n",
      "Epoch: 48, Loss: 0.684872\n",
      "Epoch: 49, Loss: 1.233255\n",
      "Epoch: 50, Loss: 0.955525\n",
      "Epoch: 51, Loss: 0.884563\n",
      "Epoch: 52, Loss: 0.635976\n",
      "Epoch: 53, Loss: 0.535951\n",
      "Epoch: 54, Loss: 0.581201\n",
      "Epoch: 55, Loss: 0.382846\n",
      "Epoch: 56, Loss: 0.343779\n",
      "Epoch: 57, Loss: 0.720704\n",
      "Epoch: 58, Loss: 0.917043\n",
      "Epoch: 59, Loss: 0.416624\n",
      "Epoch: 60, Loss: 0.701541\n",
      "Epoch: 61, Loss: 0.743858\n",
      "Epoch: 62, Loss: 0.651975\n",
      "Epoch: 63, Loss: 0.714348\n",
      "Epoch: 64, Loss: 0.473056\n",
      "Epoch: 65, Loss: 0.558682\n",
      "Epoch: 66, Loss: 0.204254\n",
      "Epoch: 67, Loss: 0.295151\n",
      "Epoch: 68, Loss: 0.403729\n",
      "Epoch: 69, Loss: 0.506586\n",
      "Epoch: 70, Loss: 0.491957\n",
      "Epoch: 71, Loss: 0.365740\n",
      "Epoch: 72, Loss: 0.522635\n",
      "Epoch: 73, Loss: 0.415144\n",
      "Epoch: 74, Loss: 0.472173\n",
      "Epoch: 75, Loss: 0.168747\n",
      "Epoch: 76, Loss: 0.208151\n",
      "Epoch: 77, Loss: 0.563441\n",
      "Epoch: 78, Loss: 0.550688\n",
      "Epoch: 79, Loss: 0.672621\n",
      "Epoch: 80, Loss: 0.444877\n",
      "Epoch: 81, Loss: 0.262447\n",
      "Epoch: 82, Loss: 1.264537\n",
      "Epoch: 83, Loss: 0.247600\n",
      "Epoch: 84, Loss: 0.206034\n",
      "Epoch: 85, Loss: 0.702743\n",
      "Epoch: 86, Loss: 0.464836\n",
      "Epoch: 87, Loss: 0.301031\n",
      "Epoch: 88, Loss: 0.234802\n",
      "Epoch: 89, Loss: 0.180219\n",
      "Epoch: 90, Loss: 0.225894\n",
      "Epoch: 91, Loss: 0.092863\n",
      "Epoch: 92, Loss: 0.103877\n",
      "Epoch: 93, Loss: 0.177886\n",
      "Epoch: 94, Loss: 0.228283\n",
      "Epoch: 95, Loss: 0.211031\n",
      "Epoch: 96, Loss: 0.064983\n",
      "Epoch: 97, Loss: 0.102282\n",
      "Epoch: 98, Loss: 0.272363\n",
      "Epoch: 99, Loss: 0.084288\n",
      "Epoch: 100, Loss: 0.268418\n",
      "Epoch: 101, Loss: 0.110656\n",
      "Epoch: 102, Loss: 0.196368\n",
      "Epoch: 103, Loss: 0.076505\n",
      "Epoch: 104, Loss: 0.051756\n",
      "Epoch: 105, Loss: 0.292540\n",
      "Epoch: 106, Loss: 0.090688\n",
      "Epoch: 107, Loss: 0.094763\n",
      "Epoch: 108, Loss: 0.035885\n",
      "Epoch: 109, Loss: 0.108444\n",
      "Epoch: 110, Loss: 0.039380\n",
      "Epoch: 111, Loss: 0.032376\n",
      "Epoch: 112, Loss: 0.020713\n",
      "Epoch: 113, Loss: 0.012070\n",
      "Epoch: 114, Loss: 0.073469\n",
      "Epoch: 115, Loss: 0.056500\n",
      "Epoch: 116, Loss: 0.221749\n",
      "Epoch: 117, Loss: 0.014228\n",
      "Epoch: 118, Loss: 0.029506\n",
      "Epoch: 119, Loss: 0.020565\n",
      "Epoch: 120, Loss: 0.021179\n",
      "Epoch: 121, Loss: 0.028889\n",
      "Epoch: 122, Loss: 0.026943\n",
      "Epoch: 123, Loss: 0.018928\n",
      "Epoch: 124, Loss: 0.023486\n",
      "Epoch: 125, Loss: 0.468663\n",
      "Epoch: 126, Loss: 0.051810\n",
      "Epoch: 127, Loss: 0.027645\n",
      "Epoch: 128, Loss: 0.012925\n",
      "Epoch: 129, Loss: 0.022334\n",
      "Epoch: 130, Loss: 0.024713\n",
      "Epoch: 131, Loss: 0.033306\n",
      "Epoch: 132, Loss: 0.034772\n",
      "Epoch: 133, Loss: 0.013071\n",
      "Epoch: 134, Loss: 0.015236\n",
      "Epoch: 135, Loss: 0.009538\n",
      "Epoch: 136, Loss: 0.007895\n",
      "Epoch: 137, Loss: 0.009594\n",
      "Epoch: 138, Loss: 0.014215\n",
      "Epoch: 139, Loss: 0.020986\n",
      "Epoch: 140, Loss: 0.026214\n",
      "Epoch: 141, Loss: 0.009609\n",
      "Epoch: 142, Loss: 0.010261\n",
      "Epoch: 143, Loss: 0.006572\n",
      "Epoch: 144, Loss: 0.007649\n",
      "Epoch: 145, Loss: 0.009657\n",
      "Epoch: 146, Loss: 0.010738\n",
      "Epoch: 147, Loss: 0.010061\n",
      "Epoch: 148, Loss: 0.006449\n",
      "Epoch: 149, Loss: 0.010444\n",
      "Epoch: 150, Loss: 0.007531\n",
      "Epoch: 151, Loss: 0.011415\n",
      "Epoch: 152, Loss: 0.008009\n",
      "Epoch: 153, Loss: 0.006829\n",
      "Epoch: 154, Loss: 0.007416\n",
      "Epoch: 155, Loss: 0.006150\n",
      "Epoch: 156, Loss: 0.014792\n",
      "Epoch: 157, Loss: 0.002988\n",
      "Epoch: 158, Loss: 0.003128\n",
      "Epoch: 159, Loss: 0.005882\n",
      "Epoch: 160, Loss: 0.004456\n",
      "Epoch: 161, Loss: 0.002080\n",
      "Epoch: 162, Loss: 0.005154\n",
      "Epoch: 163, Loss: 0.005618\n",
      "Epoch: 164, Loss: 0.003652\n",
      "Epoch: 165, Loss: 0.002631\n",
      "Epoch: 166, Loss: 0.004173\n",
      "Epoch: 167, Loss: 0.005059\n",
      "Epoch: 168, Loss: 0.005445\n",
      "Epoch: 169, Loss: 0.001078\n",
      "Epoch: 170, Loss: 0.004871\n",
      "Epoch: 171, Loss: 0.006292\n",
      "Epoch: 172, Loss: 0.004173\n",
      "Epoch: 173, Loss: 0.005011\n",
      "Epoch: 174, Loss: 0.004653\n",
      "Epoch: 175, Loss: 0.005573\n",
      "Epoch: 176, Loss: 0.004328\n",
      "Epoch: 177, Loss: 0.003396\n",
      "Epoch: 178, Loss: 0.003911\n",
      "Epoch: 179, Loss: 0.002084\n",
      "Epoch: 180, Loss: 0.007012\n",
      "Epoch: 181, Loss: 0.003442\n",
      "Epoch: 182, Loss: 0.004043\n",
      "Epoch: 183, Loss: 0.002252\n",
      "Epoch: 184, Loss: 0.003406\n",
      "Epoch: 185, Loss: 0.007055\n",
      "Epoch: 186, Loss: 0.004751\n",
      "Epoch: 187, Loss: 0.003935\n",
      "Epoch: 188, Loss: 0.006408\n",
      "Epoch: 189, Loss: 0.004055\n",
      "Epoch: 190, Loss: 0.005277\n",
      "Epoch: 191, Loss: 0.004654\n",
      "Epoch: 192, Loss: 0.003289\n",
      "Epoch: 193, Loss: 0.002565\n",
      "Epoch: 194, Loss: 0.003702\n",
      "Epoch: 195, Loss: 0.001847\n",
      "Epoch: 196, Loss: 0.003209\n",
      "Epoch: 197, Loss: 0.003377\n",
      "Epoch: 198, Loss: 0.003209\n",
      "Epoch: 199, Loss: 0.006801\n",
      "Epoch: 200, Loss: 0.003516\n",
      "Epoch: 201, Loss: 0.002549\n",
      "Epoch: 202, Loss: 0.004444\n",
      "Epoch: 203, Loss: 0.001869\n",
      "Epoch: 204, Loss: 0.003309\n",
      "Epoch: 205, Loss: 0.002385\n",
      "Epoch: 206, Loss: 0.000973\n",
      "Epoch: 207, Loss: 0.003645\n",
      "Epoch: 208, Loss: 0.005503\n",
      "Epoch: 209, Loss: 0.002074\n",
      "Epoch: 210, Loss: 0.002961\n",
      "Epoch: 211, Loss: 0.001505\n",
      "Epoch: 212, Loss: 0.002248\n",
      "Epoch: 213, Loss: 0.002810\n",
      "Epoch: 214, Loss: 0.001645\n",
      "Epoch: 215, Loss: 0.002030\n",
      "Epoch: 216, Loss: 0.002969\n",
      "Epoch: 217, Loss: 0.001945\n",
      "Epoch: 218, Loss: 0.001648\n",
      "Epoch: 219, Loss: 0.002524\n",
      "Epoch: 220, Loss: 0.002113\n",
      "Epoch: 221, Loss: 0.002072\n",
      "Epoch: 222, Loss: 0.003322\n",
      "Epoch: 223, Loss: 0.002518\n",
      "Epoch: 224, Loss: 0.002745\n",
      "Epoch: 225, Loss: 0.002987\n",
      "Epoch: 226, Loss: 0.001906\n",
      "Epoch: 227, Loss: 0.001970\n",
      "Epoch: 228, Loss: 0.001547\n",
      "Epoch: 229, Loss: 0.001639\n",
      "Epoch: 230, Loss: 0.001561\n",
      "Epoch: 231, Loss: 0.002109\n",
      "Epoch: 232, Loss: 0.003423\n",
      "Epoch: 233, Loss: 0.002331\n",
      "Epoch: 234, Loss: 0.004265\n",
      "Epoch: 235, Loss: 0.002893\n",
      "Epoch: 236, Loss: 0.002158\n",
      "Epoch: 237, Loss: 0.001230\n",
      "Epoch: 238, Loss: 0.002117\n",
      "Epoch: 239, Loss: 0.001227\n",
      "Epoch: 240, Loss: 0.002409\n",
      "Epoch: 241, Loss: 0.001879\n",
      "Epoch: 242, Loss: 0.001544\n",
      "Epoch: 243, Loss: 0.003279\n",
      "Epoch: 244, Loss: 0.002146\n",
      "Epoch: 245, Loss: 0.001464\n",
      "Epoch: 246, Loss: 0.001745\n",
      "Epoch: 247, Loss: 0.000913\n",
      "Epoch: 248, Loss: 0.004269\n",
      "Epoch: 249, Loss: 0.002417\n",
      "Epoch: 250, Loss: 0.001442\n",
      "Epoch: 251, Loss: 0.001648\n",
      "Epoch: 252, Loss: 0.001787\n",
      "Epoch: 253, Loss: 0.001064\n",
      "Epoch: 254, Loss: 0.001010\n",
      "Epoch: 255, Loss: 0.002082\n",
      "Epoch: 256, Loss: 0.000884\n",
      "Epoch: 257, Loss: 0.002935\n",
      "Epoch: 258, Loss: 0.001313\n",
      "Epoch: 259, Loss: 0.001870\n",
      "Epoch: 260, Loss: 0.003327\n",
      "Epoch: 261, Loss: 0.001402\n",
      "Epoch: 262, Loss: 0.001692\n",
      "Epoch: 263, Loss: 0.001343\n",
      "Epoch: 264, Loss: 0.001042\n",
      "Epoch: 265, Loss: 0.001711\n",
      "Epoch: 266, Loss: 0.001552\n",
      "Epoch: 267, Loss: 0.002564\n",
      "Epoch: 268, Loss: 0.001474\n",
      "Epoch: 269, Loss: 0.000686\n",
      "Epoch: 270, Loss: 0.002680\n",
      "Epoch: 271, Loss: 0.002192\n",
      "Epoch: 272, Loss: 0.002246\n",
      "Epoch: 273, Loss: 0.001773\n",
      "Epoch: 274, Loss: 0.001073\n",
      "Epoch: 275, Loss: 0.002191\n",
      "Epoch: 276, Loss: 0.001142\n",
      "Epoch: 277, Loss: 0.002218\n",
      "Epoch: 278, Loss: 0.001840\n",
      "Epoch: 279, Loss: 0.001816\n",
      "Epoch: 280, Loss: 0.001150\n",
      "Epoch: 281, Loss: 0.000891\n",
      "Epoch: 282, Loss: 0.001217\n",
      "Epoch: 283, Loss: 0.000816\n",
      "Epoch: 284, Loss: 0.001008\n",
      "Epoch: 285, Loss: 0.000944\n",
      "Epoch: 286, Loss: 0.001725\n",
      "Epoch: 287, Loss: 0.001125\n",
      "Epoch: 288, Loss: 0.001571\n",
      "Epoch: 289, Loss: 0.000685\n",
      "Epoch: 290, Loss: 0.001411\n",
      "Epoch: 291, Loss: 0.000570\n",
      "Epoch: 292, Loss: 0.000803\n",
      "Epoch: 293, Loss: 0.000782\n",
      "Epoch: 294, Loss: 0.001066\n",
      "Epoch: 295, Loss: 0.001332\n",
      "Epoch: 296, Loss: 0.001636\n",
      "Epoch: 297, Loss: 0.000659\n",
      "Epoch: 298, Loss: 0.000873\n",
      "Epoch: 299, Loss: 0.001599\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model_b(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.54\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            batch_size = imgs.shape[0]\n",
    "            outputs = model_b(imgs.view(batch_size, -1))\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "    print(\"Accuracy: {:.2f}\".format(correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0eb5d0a65b500759bcde1c4c1ad0551eaece71d5bef76353acf57400c52edb49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
